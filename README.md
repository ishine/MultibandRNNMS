<div align="center">

# RNN_MS-PyTorch <!-- omit in toc -->
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][notebook]
[![Paper](http://img.shields.io/badge/paper-arxiv.1811.06292-B31B1B.svg)][paper]  

</div>

Neural vocoder **"RNN_MS"** with PyTorch.

<!-- generated by [Markdown All in One](https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one) -->
- [Demo](#demo)
- [Quick training](#quick-training)
- [How to Use](#how-to-use)
- [System Details](#system-details)
- [Results](#results)
- [Original paper](#original-paper)

![network](network.png?raw=true "Robust Universal Neural Vocoding")

This repository is PyTorch-Lightning re-implementation of [bshall/UniversalVocoding].  
Almost all code are rewriten.  

## Demo
<!-- [Audio sample page](https://tarepan.github.io/UniversalVocoding).   -->
ToDO: Link super great impressive high-quatity audio demo.  

## Quick training
Jump to â˜ž [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][notebook], then Run. That's all!  

## How to Use
### 1. Install <!-- omit in toc -->

```bash
pip install git+https://github.com/tarepan/UniversalVocoding
```

### 2. Data & Preprocessing <!-- omit in toc -->
"Batteries Included" ðŸ˜‰  
`dataset` transparently download corpus and preprocess it for you.  

### 3. Train <!-- omit in toc -->
```bash
python -m rnnms.main_train
```

For arguments, check [./rnnms/config.py](https://github.com/tarepan/UniversalVocoding/blob/main/rnnms/config.py)  

### Advanced: Custom dataset <!-- omit in toc -->
Copy [`rnnms.main_train`] and replace DataModule.  

```python
    # datamodule = LJSpeechDataModule(batch_size, ...)
    datamodule = YourSuperCoolDataModule(batch_size, ...)
    # That's all!
```

[`rnnms.main_train`]:https://github.com/tarepan/UniversalVocoding/blob/main/rnnms/main_train.py

## System Details
### Model <!-- omit in toc -->
- PreNet: 2-layer bidi-GRU
- Upsampler: x200 time-directional latent upsampling with interpolation
- Decoder: Latent-conditional, embedded-auto-regressive generative RNN with 10-bit Î¼-law encoding

### Differences from the Paper <!-- omit in toc -->

| property      |  paper           | this repo       |
|:--------------|:-----------------|:----------------|
| sampling rate | 24 kHz           |   16 kHz        |
| AR input      | one-hot          | embedding       |
| Dataset       | internal? 74 spk | LJSpeech, 1 spk |
| Presicion     |   -              | 32/16 Mixed     |

## Results
### Output Sample <!-- omit in toc -->
[Demo](#demo)

### Performance <!-- omit in toc -->
1.1 [iter/sec] @ NVIDIA T4 on Google Colaboratory (AMP+, num_workers=8)  
(1.1 [iter/sec] with [bshall/UniversalVocoding], same setup)  

### Knowledge from Original Repository <!-- omit in toc -->
- training speed
  - config file
    - n_steps: 150k (~1.6 day with 1.1 it/s)
- input spectrogram [issue#4](https://github.com/bshall/UniversalVocoding/issues/4)
  - more "smoothed" spectrogram could be used
    - demo of VQ-VAE output (smoothed spec) => RNN_MS => .wav
- sensitivity to spectrogram shape [issue#3](https://github.com/bshall/UniversalVocoding/issues/3)
  - stable training regardless of shape
    - n_fft=1024 also work well
- old 9-bit model
  - training speed [issue#5](https://github.com/bshall/UniversalVocoding/issues/5)
    - intelligible samples by 20k steps
    - decent results by 60k-80k steps
    - no data of father step training
  - other dataset [issue#2](https://github.com/bshall/UniversalVocoding/issues/2)
    - only ZeroSpeech2019, not yet (seems to be interested in other dataset?)

## Original paper
[![Paper](http://img.shields.io/badge/paper-arxiv.1811.06292-B31B1B.svg)][paper]  
<!-- https://arxiv2bibtex.org/?q=1811.06292&format=bibtex -->
```
@misc{1811.06292,
Author = {Jaime Lorenzo-Trueba and Thomas Drugman and Javier Latorre and Thomas Merritt and Bartosz Putrycz and Roberto Barra-Chicote and Alexis Moinet and Vatsal Aggarwal},
Title = {Towards achieving robust universal neural vocoding},
Year = {2018},
Eprint = {arXiv:1811.06292},
}
```

## Acknowlegements <!-- omit in toc -->
- [bshall/UniversalVocoding]
  - Model and hyperparams are totally based on this repository.

## Dependency Notes <!-- omit in toc -->
### PyTorch version <!-- omit in toc -->
PyTorch version: PyTorch v1.9 is working (We checked with v1.9.0).  

For dependency resolution, we do **NOT** explicitly specify the compatible versions.  
PyTorch have several distributions for various environment (e.g. compatible CUDA version.)  
Unfortunately it make dependency version management complicated for dependency management system.  
In our case, the system `poetry` cannot handle cuda variant string (e.g. `torch>=1.6.0` cannot accept `1.6.0+cu101`.)  
In order to resolve this problem, we use `torch==*`, it is equal to no version specification.  
`Setup.py` could resolve this problem (e.g. `torchaudio`'s `setup.py`), but we will not bet our effort to this hacky method.  

[paper]:https://arxiv.org/abs/1811.06292
[notebook]:https://colab.research.google.com/github/tarepan/UniversalVocoding/blob/main/rnnms.ipynb
[bshall/UniversalVocoding]:https://github.com/bshall/UniversalVocoding

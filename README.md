<div align="center">

# RNN_MS-PyTorch <!-- omit in toc -->
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][notebook]
[![Paper](http://img.shields.io/badge/paper-arxiv.1811.06292-B31B1B.svg)][paper]  

</div>

Neural vocoder **"RNN_MS"** with PyTorch.

<!-- generated by [Markdown All in One](https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one) -->
- [Demo](#demo)
- [How to Use](#how-to-use)
- [System Details](#system-details)
- [Results](#results)
- [Original paper](#original-paper)

![network](network.png?raw=true "Robust Universal Neural Vocoding")

This repository is PyTorch-Lightning re-implementation of [bshall/UniversalVocoding].  
Almost all code are rewriten.  

## Demo
<!-- [Audio sample page](https://tarepan.github.io/UniversalVocoding).   -->
ToDO: Link super great impressive high-quatity audio demo.  

## How to Use
### Quick training <!-- omit in toc -->
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][notebook]

### Install <!-- omit in toc -->

```bash
!pip install git+https://github.com/tarepan/UniversalVocoding#main -q
```

### Training <!-- omit in toc -->
```bash
!python -m rnnms.main_train
```

For arguments, check [./rnnms/args.py](https://github.com/tarepan/UniversalVocoding/blob/main/rnnms/args.py)  

## System Details
### Model <!-- omit in toc -->
- Encoder: 2-layer bidi-GRU (so that no time-directional compression)
- Upsampler: x200 time-directional latent upsampling with interpolation
- Decoder: Latent-conditional, embedded-auto-regressive generative RNN with 10-bit Î¼-law encoding

### Differences from the Paper <!-- omit in toc -->

| property      |  paper           | this repo       |
|:--------------|:-----------------|:----------------|
| sampling rate | 24 kHz           |   16 kHz        |
| AR input      | one-hot          | embedding       |
| Dataset       | internal? 74 spk | LJSpeech, 1 spk |
| Presicion     |   -              | 32/16 Mixed     |

## Results
### Performance <!-- omit in toc -->
Google Colaboratory Tesla T4  
--num_workers=8 (same as bshall)  

1.1 it/s (c.f. bshall/UniversalVocoding 1.1 it/s with AMP)  

### Knowledge from Original Repository <!-- omit in toc -->
- training speed
  - config file
    - n_steps: 150k (~1.6 day with 1.1 it/s)
- input spectrogram [issue#4](https://github.com/bshall/UniversalVocoding/issues/4)
  - more "smoothed" spectrogram could be used
    - demo of VQ-VAE output (smoothed spec) => RNN_MS => .wav
- sensitivity to spectrogram shape [issue#3](https://github.com/bshall/UniversalVocoding/issues/3)
  - stable training regardless of shape
    - n_fft=1024 also work well
- old 9-bit model
  - training speed [issue#5](https://github.com/bshall/UniversalVocoding/issues/5)
    - intelligible samples by 20k steps
    - decent results by 60k-80k steps
    - no data of father step training
  - other dataset [issue#2](https://github.com/bshall/UniversalVocoding/issues/2)
    - only ZeroSpeech2019, not yet (seems to be interested in other dataset?)

## Original paper
[![Paper](http://img.shields.io/badge/paper-arxiv.1811.06292-B31B1B.svg)][paper]  
<!-- https://arxiv2bibtex.org/?q=1811.06292&format=bibtex -->
```
@misc{1811.06292,
Author = {Jaime Lorenzo-Trueba and Thomas Drugman and Javier Latorre and Thomas Merritt and Bartosz Putrycz and Roberto Barra-Chicote and Alexis Moinet and Vatsal Aggarwal},
Title = {Towards achieving robust universal neural vocoding},
Year = {2018},
Eprint = {arXiv:1811.06292},
}
```

## Acknowlegements <!-- omit in toc -->
- [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)
  - Alternative WaveRNN
- [bshall/UniversalVocoding]
  - Model and hyperparams are totally based on this repository.

## Dependency Notes <!-- omit in toc -->
### PyTorch version <!-- omit in toc -->
PyTorch version: PyTorch v1.6 is working (We checked with v1.6.0).  

For dependency resolution, we do **NOT** explicitly specify the compatible versions.  
PyTorch have several distributions for various environment (e.g. compatible CUDA version.)  
Unfortunately it make dependency version management complicated for dependency management system.  
In our case, the system `poetry` cannot handle cuda variant string (e.g. `torch>=1.6.0` cannot accept `1.6.0+cu101`.)  
In order to resolve this problem, we use `torch==*`, it is equal to no version specification.  
`Setup.py` could resolve this problem (e.g. `torchaudio`'s `setup.py`), but we will not bet our effort to this hacky method.  

[paper]:https://arxiv.org/abs/1811.06292
[notebook]:https://colab.research.google.com/github/tarepan/UniversalVocoding/blob/main/rnnms.ipynb
[bshall/UniversalVocoding]:https://github.com/bshall/UniversalVocoding